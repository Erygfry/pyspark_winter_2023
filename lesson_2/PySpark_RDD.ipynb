{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQvwElqGWoF1",
    "outputId": "b6538d7b-2559-4c42-e5f3-56e810a09165"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark findspark nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQJQt0g6e_Cj",
    "outputId": "43d6935e-767d-4576-fcc1-ede94d9cb3a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Petr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Petr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      2\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mSPARK_LOCAL_IP\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m127.0.0.1\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# важный фикс для локали/WSL/Windows\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      6\u001b[39m spark = (\n\u001b[32m      7\u001b[39m     \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal-fast\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# локальный режим, все ядра\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.ui.port\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4050\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.serializer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.serializer.KryoSerializer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.dynamicAllocation.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfalse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# отключаем в локале\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.shuffle.service.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfalse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# отключаем в локале\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.bindAddress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# не ждём странный DNS\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.host\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m sc = spark.sparkContext\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(spark.version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petr\\PycharmProjects\\pyspark_winter_2023\\venv\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petr\\PycharmProjects\\pyspark_winter_2023\\venv\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petr\\PycharmProjects\\pyspark_winter_2023\\venv\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petr\\PycharmProjects\\pyspark_winter_2023\\venv\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petr\\PycharmProjects\\pyspark_winter_2023\\venv\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"  # важный фикс для локали/WSL/Windows\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"local-fast\")\n",
    "    .master(\"local[*]\")  # локальный режим, все ядра\n",
    "    .config(\"spark.ui.port\", \"4050\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")  # отключаем в локале\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\")  # отключаем в локале\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")  # не ждём странный DNS\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvkEEKu5e-Th"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[32m      4\u001b[39m conf = SparkConf().set(\u001b[33m'\u001b[39m\u001b[33mspark.ui.port\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m4050\u001b[39m\u001b[33m'\u001b[39m).set(\u001b[33m'\u001b[39m\u001b[33mspark.serializer\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33morg.apache.spark.serializer.KryoSerializer\u001b[39m\u001b[33m'\u001b[39m)\\\n\u001b[32m      5\u001b[39m                   .set(\u001b[33m'\u001b[39m\u001b[33mspark.dynamicAllocation.enabled\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m)\\\n\u001b[32m      6\u001b[39m                   .set(\u001b[33m'\u001b[39m\u001b[33mspark.shuffle.service.enabled\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m#трекер, чтобы возвращать ресурсы\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m spark = SparkSession.builder.master(\u001b[33m'\u001b[39m\u001b[33mlocal[*]\u001b[39m\u001b[33m'\u001b[39m).getOrCreate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petr\\PycharmProjects\\pyspark_winter_2023\\venv\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petr\\PycharmProjects\\pyspark_winter_2023\\venv\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petr\\PycharmProjects\\pyspark_winter_2023\\venv\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.ui.port\", \"4050\")\n",
    "    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    ")  # трекер, чтобы возвращать ресурсы\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysb-yADyseDE"
   },
   "source": [
    "Как проще всего создать RDD? Вызвать метод и передать ему нужный объект\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "459H4QvujHKR"
   },
   "outputs": [],
   "source": [
    "first_rdd = sc.parallelize(range(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRhiwucq51Sl",
    "outputId": "596c7a7c-f394-4f63-d632-19f003d73e7a"
   },
   "outputs": [],
   "source": [
    "first_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmi41EwXy1gI"
   },
   "source": [
    "А можно ли менять количество партиций? Да, для этого есть два метода: repartition() и coalesce(). Первый используется для увеличения и уменьшения количества партиций, второй только для снижения, прчем coalesce будет работать эффективнее. Много партиций - дольше будет считаться, но если данных много, то обязательно нужно\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajdioFzEsgZB"
   },
   "source": [
    "repartition()`всегда приводит к равномерному перераспределению данных, что ведет к shuffle. Если Вы уменьшаете число партиций, то стоит использовать`coalesce()`, который может избежать shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTL0MygY593s",
    "outputId": "5ee2d63f-c1e1-4410-c82e-1951e06d4c92"
   },
   "outputs": [],
   "source": [
    "first_rdd = first_rdd.repartition(5)\n",
    "print(first_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S72uRYdGtz6i",
    "outputId": "b6c307e8-6818-4191-8007-54d4b77117b9"
   },
   "outputs": [],
   "source": [
    "first_rdd = first_rdd.repartition(2)\n",
    "print(first_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6k45ZdHFt1uU",
    "outputId": "71bcbc56-32e2-491f-fee5-4b2cc81cad91"
   },
   "outputs": [],
   "source": [
    "first_rdd = first_rdd.coalesce(1)\n",
    "print(first_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOjksB0guT4k",
    "outputId": "49186c1e-2e33-48fc-ef27-14c247632485"
   },
   "outputs": [],
   "source": [
    "a = %time first_rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i63A8-0uuDJ2"
   },
   "source": [
    "Посмотрим на время выполнения для разного числа партиций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Os4QBpNTuBvk",
    "outputId": "04597f53-cebc-44d4-9213-214584fb7ef4"
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "first_rdd  = sc.parallelize(range(5000000))\n",
    "for partition in range(1, 30):\n",
    "    first_rdd = first_rdd.repartition(partition)\n",
    "    time = %timeit -o  -n 1 -r 1 first_rdd.sum()\n",
    "    time = time.best\n",
    "    result.append((partition, time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tICS1hZBxAYT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "aMfCB5CLxFWP",
    "outputId": "516bd630-5e3f-42c9-d600-adc2728ab2b8"
   },
   "outputs": [],
   "source": [
    "plt.plot([res[1] for res in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKB8WZZ3z1if"
   },
   "source": [
    "А почему лучший результат при небольшом количестве партиций?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuiJd2Oqz0yY",
    "outputId": "0a6ac64a-5525-4c2a-86c2-153309754da6"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s8tP7e-y_ib"
   },
   "source": [
    "Из list также можно создавать RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xo5auCMsy7aH"
   },
   "outputs": [],
   "source": [
    "bad_list = [1, 2, 3, \"a\", 10, \"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCpu1vIWzHYT"
   },
   "outputs": [],
   "source": [
    "bad_list_rdd = sc.parallelize(bad_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFHIC38GzbAT",
    "outputId": "1686512f-6393-4843-aa0b-60db63b2268d"
   },
   "outputs": [],
   "source": [
    "bad_list_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gQxsxLd0UTH"
   },
   "source": [
    "Еще можно создать RDD через textFile и wholeTextFiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l5oE92a1q2z"
   },
   "outputs": [],
   "source": [
    "text_rdd = sc.textFile(\"spark_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FKzdhuh10Z2",
    "outputId": "58f4b5ab-9431-438d-e9b6-f5bbe96135e2"
   },
   "outputs": [],
   "source": [
    "text_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "065n4emo2Kuf"
   },
   "source": [
    "wholeTextFiles создает PairRDD в формате key-value, где ключ - имя файла, а значения - то, что находистя в файле. Имена файлов считываются из папки через wholeTextFiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTGV-Tk42nNT"
   },
   "outputs": [],
   "source": [
    "dirPath = \"files\"\n",
    "os.mkdir(dirPath)\n",
    "with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
    "    _ = file1.write(\"[1 2 3]\")\n",
    "with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
    "    _ = file2.write(\"[4 5 6]\")\n",
    "textFiles = sc.wholeTextFiles(dirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2odeXS186wbv",
    "outputId": "526bb8df-6731-417c-d4c3-b6b2e12a0f0e"
   },
   "outputs": [],
   "source": [
    "textFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q49V_P0M6fV-",
    "outputId": "333fa064-d46b-4bd0-b23f-8264b0eb4768"
   },
   "outputs": [],
   "source": [
    "textFiles.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUEQ7fTh7KIg"
   },
   "source": [
    "У RDD есть стандартно 2 типа методов - actions и transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9xpZ-Ns7b-b"
   },
   "source": [
    "**Actions**\n",
    "\n",
    "Начнем с actions, то есть того, что заставит посчитать\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xn4EpCXJ7WWo"
   },
   "outputs": [],
   "source": [
    "first_rdd = first_rdd.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMUv2tyh7ob4",
    "outputId": "6cb2eac0-3904-4dd3-d544-2b6cd70ac38c"
   },
   "outputs": [],
   "source": [
    "first_rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUDdK1co7qQc",
    "outputId": "40d81031-2c94-4d95-d086-693f9ee19246"
   },
   "outputs": [],
   "source": [
    "first_rdd.min(), first_rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKijZqmp8uir",
    "outputId": "b2f7a841-1d85-4627-bb41-3c81f5329f43"
   },
   "outputs": [],
   "source": [
    "first_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUnqk-QI7wqE",
    "outputId": "6d9274f9-2c9f-4555-e170-25803aba770f"
   },
   "outputs": [],
   "source": [
    "first_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUZk1eg87zky",
    "outputId": "90038325-e83a-4c82-cb39-9170a2239df4"
   },
   "outputs": [],
   "source": [
    "first_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMCFnJjG76Q-"
   },
   "outputs": [],
   "source": [
    "a = first_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh1IT2cP8BPj"
   },
   "outputs": [],
   "source": [
    "first_rdd.saveAsTextFile, first_rdd.saveAsPickleFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFXlzHqA8T1M",
    "outputId": "dcd6c160-f40a-472e-da0e-78e17c32710b"
   },
   "outputs": [],
   "source": [
    "first_rdd.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev48GQfDqnrS"
   },
   "source": [
    "Если нужно получить небольшое число записей на драйвер и, при этом, сохранить распределение, то лучше сделать выборку\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDHNH3H19Pib",
    "outputId": "1e2d95b3-8e4b-47ed-90cf-378e388b529a"
   },
   "outputs": [],
   "source": [
    "first_rdd.takeSample(withReplacement=False, num=5, seed=5757)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aiad4-a8ejvL"
   },
   "source": [
    "**Transformations**\n",
    "\n",
    "Это просто трансформации, которые не будут вычисляться до вызова actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMpdFk3W9P-E"
   },
   "outputs": [],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "b = sc.parallelize([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmJMLmCO9Xi2"
   },
   "outputs": [],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X196_-ZQ9arZ",
    "outputId": "59d7e3a0-e2ae-4eae-eae5-f02e751565e8"
   },
   "outputs": [],
   "source": [
    "c.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQNtnHNtfzEC"
   },
   "source": [
    "filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7qMFVREfxv8",
    "outputId": "fd7097ea-2e5d-444f-e8d8-2075f947a8b0"
   },
   "outputs": [],
   "source": [
    "text_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wiJ08Frf-Jz",
    "outputId": "2ca426c3-e371-4e37-ae62-685f6d3f515d"
   },
   "outputs": [],
   "source": [
    "text_rdd.filter(lambda x: x != \"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYQT0S4ugRRp"
   },
   "outputs": [],
   "source": [
    "text_rdd = text_rdd.filter(lambda x: x != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E41esva9ex2j"
   },
   "source": [
    "map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQ5yWa7yfUVv"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gny7-CMYgkQQ"
   },
   "outputs": [],
   "source": [
    "def mapper_text(text):\n",
    "    clean_text = re.sub(rf\"[{string.punctuation}]\", \"\", text)\n",
    "    words = nltk.word_tokenize(clean_text)\n",
    "    words_with_value = [(word.lower(), 1) for word in words if word not in stop_words]\n",
    "    return words_with_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-C7K6V3Sgw4-",
    "outputId": "66e8f456-1f9f-404f-fc9a-2399186e3329"
   },
   "outputs": [],
   "source": [
    "text_rdd.map(mapper_text).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw6eE6BnhC4G"
   },
   "source": [
    "flatMap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTUOQWrZhS89"
   },
   "source": [
    "Попробуем применить map и flatMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Zi_fYGfhEza",
    "outputId": "fabc51e4-41be-43b4-d15a-838f1bad5844"
   },
   "outputs": [],
   "source": [
    "text_rdd.map(mapper_text).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUp5kQAnhMsW",
    "outputId": "3aa080cc-8cad-4636-d291-3ac210473b3a"
   },
   "outputs": [],
   "source": [
    "text_rdd.flatMap(mapper_text).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWVR-ueohXQ-"
   },
   "source": [
    "Как так?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKf-8WfLhscS",
    "outputId": "faf3d48c-e596-48c6-c210-8b2a1a194271"
   },
   "outputs": [],
   "source": [
    "text_rdd.map(mapper_text).map(len).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm1g9-0EiN2x"
   },
   "source": [
    "вроде понятно что случилось, но давайте на игрушечном примере\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwUcamJHiNJa"
   },
   "outputs": [],
   "source": [
    "simple_example = sc.parallelize([[1, 2, 3], [2, 3, 4], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e95UYZri0P1"
   },
   "outputs": [],
   "source": [
    "def pow_elements(elements):\n",
    "    return [x**2 for x in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRuQNwphilD7",
    "outputId": "b5b59b28-3e47-4f8c-b422-4294855b8684"
   },
   "outputs": [],
   "source": [
    "simple_example.map(pow_elements).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JQ0zRRmjC6Y",
    "outputId": "eefc2e9f-f7cf-451c-96df-53f9ad905b85"
   },
   "outputs": [],
   "source": [
    "simple_example.flatMap(pow_elements).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYbF8PgtkA5U"
   },
   "source": [
    "groupByKey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbY1SU54j_Ln"
   },
   "outputs": [],
   "source": [
    "text_rdd = text_rdd.flatMap(mapper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSAVshc5jVlO",
    "outputId": "8e520a47-972d-4282-df1f-147234ed64ca"
   },
   "outputs": [],
   "source": [
    "text_rdd.groupByKey().mapValues(len).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGHXTxQElEki"
   },
   "source": [
    "sortByKey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZS4CyLFVlGWe",
    "outputId": "b10bf4ae-3682-4bd4-e059-3c6063336404"
   },
   "outputs": [],
   "source": [
    "text_rdd.groupByKey().mapValues(len).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfqBo3xRlYbA"
   },
   "source": [
    "И так на самом деле много методов, но предалагаю написать подсчет частоты слов и сделаем это в стиле программ на java\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpcSn6-hllUm"
   },
   "outputs": [],
   "source": [
    "text_rdd = sc.textFile(\"spark_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNKk3doilxe3"
   },
   "outputs": [],
   "source": [
    "result = (\n",
    "    text_rdd.filter(lambda x: x != \"\")\n",
    "    .flatMap(mapper_text)\n",
    "    .groupByKey()\n",
    "    .mapValues(len)\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1tJ2dCcmh8z",
    "outputId": "099d0bbf-5fd1-40f0-de4e-bf9f8614ad82"
   },
   "outputs": [],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDSTE1oloeOq"
   },
   "source": [
    "Забыли про reduceByKey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpv46Uxiobm8",
    "outputId": "286c1821-bfcf-4c39-c0cc-22005e6c8685"
   },
   "outputs": [],
   "source": [
    "text_rdd.filter(lambda x: x != \"\").flatMap(mapper_text).reduceByKey(\n",
    "    lambda x, y: x + y\n",
    ").sortBy(lambda x: x[1], ascending=False).collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PxSwc7QrVk7"
   },
   "source": [
    "Стоит заметить, что `groupByKey()` предполагает перемещение всех записей с одним ключом на один экзекьютор. В случае очень скошенных распределений это может привести к падению экзекьютора с OOM. Поэтому всегда при группировках стоит подумать об использовании `reduceByKey()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlu6iY8qGYlw"
   },
   "source": [
    "Так, на лекции было что-то про count, который не делает shuffle да и вообще можно проще написать?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-esll2IFI3M"
   },
   "outputs": [],
   "source": [
    "def mapper_text_simple(text):\n",
    "    clean_text = re.sub(rf\"[{string.punctuation}]\", \"\", text)\n",
    "    words = nltk.word_tokenize(clean_text)\n",
    "    words = [word.lower() for word in words if word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gs_zfIJsT3f",
    "outputId": "bf26c370-d09c-4179-85c7-e3ac7aa9401c"
   },
   "outputs": [],
   "source": [
    "result = text_rdd.filter(lambda x: x != \"\").flatMap(mapper_text_simple).countByValue()\n",
    "result = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7FoUF1mqImQ"
   },
   "source": [
    "Замеры\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBdtooISqQ8U",
    "outputId": "7f9bd4a7-f418-49f8-e304-dee864ba1595"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "text_rdd.filter(lambda x: x != '')\\\n",
    "        .flatMap(mapper_text)\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(len)\\\n",
    "        .sortBy(lambda x: x[1], ascending=False)\\\n",
    "        .collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdAdFM-rqHxX",
    "outputId": "dbfda922-02e8-4787-df73-c2a38a1af194"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "text_rdd.filter(lambda x: x != '')\\\n",
    "        .flatMap(mapper_text)\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .sortBy(lambda x: x[1], ascending=False)\\\n",
    "        .collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xD4ne4tmqfAI",
    "outputId": "a47eb726-ecb3-483b-d535-f9219aa78d5e"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "result = text_rdd.filter(lambda x: x != '')\\\n",
    "                 .flatMap(mapper_text_simple)\\\n",
    "                 .countByValue()\n",
    "result = sorted(result.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNLF_SEaG_Mr"
   },
   "source": [
    "**Join'ы**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At9Da6ZlHIcs"
   },
   "source": [
    "Тут просто на игрушечном примере пощупаем данную операцию\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HQ4wqICHPNK"
   },
   "outputs": [],
   "source": [
    "rdd_a = sc.parallelize([(\"a\", [1, 2]), (\"b\", [2, 4])])\n",
    "\n",
    "rdd_b = sc.parallelize([(\"a\", [10]), (\"c\", [11])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOCGK6dVHqCG",
    "outputId": "d88ee2cc-9d44-44e9-b6dc-bd6b4f989989"
   },
   "outputs": [],
   "source": [
    "rdd_a.join(rdd_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmnzobldH68k",
    "outputId": "d3ab4d79-e6f5-4923-e5b3-b78813e60a5f"
   },
   "outputs": [],
   "source": [
    "rdd_a.leftOuterJoin(rdd_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nS-DcoMEH-Mq",
    "outputId": "e9ab17cc-5682-4986-a28c-eea0b207c776"
   },
   "outputs": [],
   "source": [
    "rdd_a.fullOuterJoin(rdd_b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_cvDn0sI6lS"
   },
   "source": [
    "**Домашнее задание 1**\n",
    "\n",
    "Посчитать количество рейтингов больше 4 для каждого фильма и вывести фильмы в порядке убывания количества этих оценок (можно вывести топ 10)\n",
    "\n",
    "Файл можете взять из прошлого домашнего задания + сохраните результат на диск\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.ui.port\", \"4050\")\n",
    "    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    ")  # трекер, чтобы возвращать ресурсы\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_rdd = (\n",
    "    spark.read.option(\"delimiter\", \"\\t\")\n",
    "    .csv(\n",
    "        \"/content/drive/MyDrive/pyspark_winter_2023/lesson_2/user_ratedmovies.dat\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    .select(\"userID\", \"movieID\", \"rating\")\n",
    ").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hw_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = hw_rdd.filter(lambda x: x[2] > 4.0).map(\n",
    "    lambda x: (x[1], 1)\n",
    ")  # фильтруем (пиво) фильмы с оценкой выше 4 и приводим к нужному виду\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b).sortBy(\n",
    "    lambda x: x[1], ascending=False\n",
    ")  # группируем по id фильма и сортируем по количеству вхождений\n",
    "counts_df = counts.toDF([\"movieID\", \"count\"])  # приводим к датафрему чтобы было красиво\n",
    "counts_df.show(5)  # выводим топ 5 фильмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BpNsZDmJQm2"
   },
   "source": [
    "**Домашнее задание 2**\n",
    "\n",
    "В этом задании у вас есть файл с обученным word2vec на произведении Достоевского Преступление и наказание. Файл - list, каждый элемент которого слово и его вектор в формате ('word', [vector]). Необходимо для каждого слова собрать список его top 10 похожих слов по косинусной метрике\n",
    "Результат также сохраните на диск и выведите синонимы для слова 'топор' и 'деньга'.\n",
    "Файл в пикле, так что для считывания воспользуйтесь не spark, можете взять любимый pandas\n",
    "\n",
    "Файл не очень уж и маленький, рекомендую сначала написать код на кошках/собачках, а потом уже на всем, так как ядра 2, считаться будет очень долго.\n",
    "В качестве одного из вариантов можете рассмотреть метод cartesian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "data = pd.read_pickle(\"w2v_vectors.pickle\")\n",
    "display(len(data), type(data), len(data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vec: list):\n",
    "    v = [float(scalar) for scalar in vec]\n",
    "    s = math.sqrt(sum(a * a for a in v))\n",
    "    return [a / s for a in v] if s else [0.0] * len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = sc.parallelize(data).map(lambda t: (t[0], normalize(t[1])))\n",
    "pairs = (\n",
    "    words_rdd.cartesian(words_rdd)\n",
    "    .filter(lambda a_b: a_b[0][0] != a_b[1][0])\n",
    "    .map(\n",
    "        lambda a_b: (\n",
    "            a_b[0][0],\n",
    "            (a_b[1][0], sum([x * y for x, y in zip(a_b[0][1], a_b[1][1])])),\n",
    "        )\n",
    "    )\n",
    "    .filter(lambda x: x[0] in [\"топор\", \"деньга\"])\n",
    "    .sortBy(lambda x: x[1][1], ascending=False)\n",
    ")\n",
    "\n",
    "pairs.take(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
